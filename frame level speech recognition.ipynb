{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Frame level speech recognition\n## Ayush Kumar (MS19038)","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nfrom sklearn.metrics import accuracy_score\nimport random\n\n\nfrom torch.utils import data\nfrom torchvision import transforms\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport time\n\n# random seed\nnp.random.seed(11785)\ntorch.manual_seed(11785)\n\ncuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if cuda else \"cpu\")\nsys.version\nprint(cuda, sys.version)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T17:47:07.021635Z","iopub.execute_input":"2023-04-04T17:47:07.022113Z","iopub.status.idle":"2023-04-04T17:47:10.903288Z","shell.execute_reply.started":"2023-04-04T17:47:07.022074Z","shell.execute_reply":"2023-04-04T17:47:10.902005Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"False 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \n[GCC 9.4.0]\n","output_type":"stream"}]},{"cell_type":"code","source":"import argparse\nimport time\nimport csv\nfrom datetime import datetime\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\nfrom torchvision.models import resnet\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:36:30.158476Z","iopub.execute_input":"2023-04-04T16:36:30.159449Z","iopub.status.idle":"2023-04-04T16:36:30.164942Z","shell.execute_reply.started":"2023-04-04T16:36:30.159412Z","shell.execute_reply":"2023-04-04T16:36:30.163651Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import math\nclass VariableLengthPooling(nn.Module):\n    def forward(self, x, **kwargs):\n        bounds = kwargs.get(\"bounds\")\n        sum_bounds = torch.sum(bounds, dim=1)\n        out = torch.bmm(x, bounds) / sum_bounds\n        return out\n\ndef conv3x3(in_planes, out_planes, kernel_size=3, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                     padding=kernel_size // 2, bias=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T21:46:55.663058Z","iopub.execute_input":"2023-04-01T21:46:55.663779Z","iopub.status.idle":"2023-04-01T21:46:55.671648Z","shell.execute_reply.started":"2023-04-01T21:46:55.663737Z","shell.execute_reply":"2023-04-01T21:46:55.670173Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, kernel_size=3, stride=1, downsample=None):\n        super().__init__()\n\n        self.conv1 = conv3x3(inplanes, planes, kernel_size=kernel_size, stride=stride)\n        self.bn1 = nn.BatchNorm1d(planes)\n        self.relu = nn.LeakyReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes, kernel_size=kernel_size, stride=stride)\n        self.bn2 = nn.BatchNorm1d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T21:46:56.224096Z","iopub.execute_input":"2023-04-01T21:46:56.224917Z","iopub.status.idle":"2023-04-01T21:46:56.237462Z","shell.execute_reply.started":"2023-04-01T21:46:56.224875Z","shell.execute_reply":"2023-04-01T21:46:56.236265Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, kernel_size=3, stride=1, downsample=None):\n        super().__init__()\n        \n        self.conv1 = nn.Conv1d(in_channels=inplanes, out_channels=planes, kernel_size=1, bias=True)\n        self.bn1 = nn.BatchNorm1d(num_features=planes)\n        \n        self.conv2 = nn.Conv1d(in_channels=planes, out_channels=planes, kernel_size=kernel_size, stride=stride,\n                               padding=kernel_size//2, bias=True)\n        self.bn2 = nn.BatchNorm1d(num_features=planes)\n        \n        self.conv3 = nn.Conv1d(in_channels=planes, out_channels=planes * self.expansion, kernel_size=1, bias=True)\n        self.bn3 = nn.BatchNorm1d(num_features=planes * self.expansion)\n        \n        self.relu = nn.LeakyReLU(inplace=True)\n        \n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T21:46:56.655568Z","iopub.execute_input":"2023-04-01T21:46:56.656015Z","iopub.status.idle":"2023-04-01T21:46:56.669040Z","shell.execute_reply.started":"2023-04-01T21:46:56.655976Z","shell.execute_reply":"2023-04-01T21:46:56.667920Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=46):\n        self.inplanes = 192\n        super(ResNet, self).__init__()\n        \n        self.conv1 = nn.Conv1d(40, 192, kernel_size=3, stride=1, padding=1, bias=True)\n        self.bn1 = nn.BatchNorm1d(192)\n        \n        self.layer0 = self._make_layer(block, 256, layers[0])\n        self.layer1 = self._make_layer(block, 256, layers[0], kernel_size=1, stride=1)\n        self.layer2 = self._make_layer(block, 256, layers[1], kernel_size=5, stride=1)\n        self.layer3 = self._make_layer(block, 256, layers[2], kernel_size=5, stride=1)\n        self.layer4 = self._make_layer(block, 512, layers[3], kernel_size=1, stride=1)\n        self.layer5 = self._make_layer(block, 512, layers[3], stride=1)\n\n        self.conv_merge = nn.Conv1d(512 * block.expansion, num_classes,\n                                    kernel_size=3, stride=1, padding=1,\n                                    bias=True)\n        self.vlp = VariableLengthPooling()\n\n    def _make_layer(self, block, planes, blocks, kernel_size=3, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv1d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=1, bias=False),\n                nn.BatchNorm1d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, kernel_size=kernel_size,\n                            stride=stride, downsample=downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, kernel_size=kernel_size))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x, bounds=None):\n        x = F.leaky_relu(self.bn1(self.conv1(x)), inplace=True)\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        x = self.conv_merge(x)\n        x = torch.squeeze(x, dim=2)\n        x = self.vlp(x, bounds=bounds)\n\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T21:46:57.027193Z","iopub.execute_input":"2023-04-01T21:46:57.028020Z","iopub.status.idle":"2023-04-01T21:46:57.045665Z","shell.execute_reply.started":"2023-04-01T21:46:57.027977Z","shell.execute_reply":"2023-04-01T21:46:57.044464Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndef to_float_tensor(numpy_array):\n    \"\"\"\n    Converts a NumPy array to a PyTorch float tensor.\n    \"\"\"\n    return torch.from_numpy(numpy_array).float()\n\ndef to_long_tensor(numpy_array):\n    \"\"\"\n    Converts a NumPy array to a PyTorch long tensor.\n    \"\"\"\n    return torch.from_numpy(numpy_array).long()\n\ndef to_tensor(numpy_array):\n    \"\"\"\n    Converts a NumPy array to a PyTorch tensor.\n    \"\"\"\n    return torch.from_numpy(numpy_array)\n\ndef to_variable(tensor):\n    \"\"\"\n    Converts a PyTorch tensor to a PyTorch variable, and moves it to the GPU if possible.\n    \"\"\"\n    if torch.cuda.is_available():\n        # Move tensor to GPU\n        tensor = tensor.cuda()\n    return torch.autograd.Variable(tensor)\n\ndef get_onehot(b, n_phones, n_frames):\n    \"\"\"\n    Generates a one-hot encoding of a sequence of frame boundaries given the number of phones and frames.\n    \"\"\"\n    b2 = np.concatenate((b[1:], [n_frames]))\n    o = np.zeros((n_frames, n_phones))\n    p = np.zeros(n_frames, dtype=int)\n    for idx, (s, e) in enumerate(zip(b, b2)):\n        p[s:e] = idx\n    o[range(n_frames), p] = 1\n    return o\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T21:46:58.448248Z","iopub.execute_input":"2023-04-01T21:46:58.448812Z","iopub.status.idle":"2023-04-01T21:46:58.466890Z","shell.execute_reply.started":"2023-04-01T21:46:58.448759Z","shell.execute_reply":"2023-04-01T21:46:58.465701Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, x, y, for_conv2d=False):\n        super().__init__()\n        self.x = x\n        self.y = y\n        self.for_conv2d = for_conv2d\n        self.total_phonemes = sum([len(xi[1]) for xi in x])\n        print(f\"n_utters {self.x.shape[0]}, total_phonemes {self.total_phonemes}\")\n\n    def __getitem__(self, idx):\n        frames = self.x[idx][0]\n        bounds = self.x[idx][1]\n        n_phones = len(bounds)\n        n_frames = len(frames)\n        bounds_onehot = self.get_onehot(bounds, n_phones, n_frames)\n        frames = frames.transpose()\n        if self.for_conv2d:\n            frames = np.expand_dims(frames, axis=0)\n        return torch.tensor(frames, dtype=torch.float32), \\\n               torch.tensor(bounds_onehot, dtype=torch.float32), \\\n               torch.tensor(self.y[idx] if self.y is not None else np.array([-1]), dtype=torch.long)\n\n    def __len__(self):\n        return self.x.shape[0]\n\n    def get_onehot(self, bounds, n_phones, n_frames):\n        b2 = np.concatenate((bounds[1:], [n_frames]))\n        o = np.zeros((n_frames, n_phones))\n        p = np.zeros(n_frames, dtype=int)\n        for idx, (s, e) in enumerate(zip(bounds, b2)):\n            p[s:e] = idx\n        o[range(n_frames), p] = 1\n        return o\n\ndef get_data_loaders(args, for_conv2d=False):\n    print(\"loading data\")\n    xtrain = np.load(f\"{args.data_dir}/train_data.npy\", allow_pickle=True)\n    ytrain = np.load(f\"{args.data_dir}/train_labels.npy\", allow_pickle=True)\n    xdev = np.load(f\"{args.data_dir}/dev_data.npy\", allow_pickle=True)\n    ydev = np.load(f\"{args.data_dir}/dev_labels.npy\", allow_pickle=True)\n\n    print(\"load complete\")\n    kwargs = {\"num_workers\": 3, \"pin_memory\": True} if args.cuda else {}\n    train_loader = DataLoader(\n        MyDataset(xtrain, ytrain, for_conv2d=for_conv2d),\n        batch_size=args.batch_size, shuffle=True, **kwargs)\n    dev_loader = DataLoader(\n        MyDataset(xdev, ydev, for_conv2d=for_conv2d),\n        batch_size=args.batch_size, shuffle=True, **kwargs)\n\n    return train_loader, dev_loader\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv1d):\n        nn.init.xavier_normal_(m.weight.data)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T21:46:59.567620Z","iopub.execute_input":"2023-04-01T21:46:59.568034Z","iopub.status.idle":"2023-04-01T21:46:59.586173Z","shell.execute_reply.started":"2023-04-01T21:46:59.567998Z","shell.execute_reply":"2023-04-01T21:46:59.584714Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# The main model\nclass MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.firstrun = True\n        self.layers = nn.ModuleList([\n            nn.Conv1d(40, 192, 3, padding=1),\n            nn.BatchNorm1d(192),\n            nn.LeakyReLU(inplace=True),\n\n            # A\n            nn.Conv1d(192, 192, 3, padding=1),\n            nn.BatchNorm1d(192),\n            nn.LeakyReLU(inplace=True),\n            nn.Conv1d(192, 192, 3, padding=1),\n            nn.BatchNorm1d(192),\n            nn.LeakyReLU(inplace=True),\n            nn.Conv1d(192, 192, 3, padding=1),\n            nn.BatchNorm1d(192),\n            nn.LeakyReLU(inplace=True),\n\n            nn.Conv1d(192, 192, 1, padding=0),\n            nn.BatchNorm1d(192),\n            nn.LeakyReLU(inplace=True),\n\n            # B\n            nn.Conv1d(192, 192, 3, padding=1),\n            nn.BatchNorm1d(192),\n            nn.LeakyReLU(inplace=True),\n            nn.Conv1d(192, 192, 3, padding=1),\n            nn.BatchNorm1d(192),\n            nn.LeakyReLU(inplace=True),\n            nn.Conv1d(192, 256, 3, padding=1),\n            nn.BatchNorm1d(256),\n            nn.LeakyReLU(inplace=True),\n\n            nn.Conv1d(256, 256, 1, padding=0),\n            nn.BatchNorm1d(256),\n            nn.LeakyReLU(inplace=True),\n            \n            # C\n            nn.Conv1d(256, 256, 3, padding=1),\n            nn.BatchNorm1d(256),\n            nn.LeakyReLU(inplace=True),\n            nn.Conv1d(256, 256, 3, padding=1),\n            nn.BatchNorm1d(256),\n            nn.LeakyReLU(inplace=True),\n            nn.Conv1d(256, 256, 3, padding=1),\n            nn.BatchNorm1d(256),\n            nn.LeakyReLU(inplace=True),\n            #\n            nn.Conv1d(256, 256, 1, padding=0),\n            nn.BatchNorm1d(256),\n            nn.LeakyReLU(inplace=True),\n\n            # D\n            nn.Conv1d(256, 512, 3, padding=1),\n            nn.BatchNorm1d(512),\n            nn.LeakyReLU(inplace=True),\n            nn.Conv1d(512, 512, 3, padding=1),\n            nn.BatchNorm1d(512),\n            nn.LeakyReLU(inplace=True),\n            nn.Conv1d(512, 512, 3, padding=1),\n            nn.BatchNorm1d(512),\n            nn.LeakyReLU(inplace=True),\n\n            nn.Conv1d(512, 512, 1, padding=0),\n            nn.BatchNorm1d(512),\n            nn.LeakyReLU(inplace=True),\n\n            # E\n            nn.Conv1d(512, 512, 5, padding=2),\n            nn.BatchNorm1d(512),\n            nn.LeakyReLU(inplace=True),\n            nn.Conv1d(512, 512, 7, padding=3),\n            nn.BatchNorm1d(512),\n            nn.LeakyReLU(inplace=True),\n            nn.Conv1d(512, 512, 9, padding=4),\n            nn.BatchNorm1d(512),\n            nn.LeakyReLU(inplace=True),\n            nn.Conv1d(512, 1024, 11, padding=5),\n            nn.BatchNorm1d(1024),\n            nn.LeakyReLU(inplace=True),\n            \n            nn.Conv1d(256, 128, 1, padding=0),\n            nn.BatchNorm1d(128),\n            nn.LeakyReLU(inplace=True),\n            nn.Conv1d(128, 128, 1, padding=0),\n            nn.BatchNorm1d(128),\n            nn.LeakyReLU(inplace=True),\n            nn.Conv1d(128, 46, 1, padding=0),\n            nn.BatchNorm1d(46),\n            nn.LeakyReLU(inplace=True),\n\n            nn.Conv1d(1024, 1024, 3, padding=1),\n            nn.BatchNorm1d(1024),\n            nn.LeakyReLU(inplace=True),\n            nn.Conv1d(1024, 1024, 3, padding=1),\n            nn.BatchNorm1d(1024),\n            nn.LeakyReLU(inplace=True),\n            nn.Conv1d(1024, 46, 3, padding=1),\n            nn.BatchNorm1d(46),\n            nn.LeakyReLU(inplace=True),\n\n            VariableLengthPooling()\n        ])\n\n    def forward(self, input, bounds=None, print_firstrun=False):\n        h = input\n        if self.firstrun:\n            print(\"****************************************\")\n            print(\"input: {}\".format(h.size()))\n        for i, layer in enumerate(self.layers):\n            if i == len(self.layers) - 1 and isinstance(layer, VariableLengthPooling):\n                h = layer(h, bounds=bounds)\n            else:\n                h = layer(h)\n            if print_firstrun and self.firstrun:\n                print(\"{}: {}\".format(layer, h.size()))\n        if self.firstrun:\n            print(\"****************************************\")\n        self.firstrun = False\n        return h\n\n\ndef MyModelResNet1D():\n    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=46)\n\ndef train(epoch, model, optimizer, train_loader, args):\n    model.train()\n\n    t0 = time.time()\n    for batch_idx, (frames, bounds, labels) in enumerate(train_loader):\n        if args.cuda:\n            frames, bounds, labels = map(lambda x: x.cuda(), [frames, bounds, labels])\n        frames, bounds, labels = map(lambda x: Variable(x), [frames, bounds, labels])\n        optimizer.zero_grad()\n\n        data = frames\n        output = model(data, bounds=bounds)\n\n        n_phones = len(labels.squeeze())\n        loss = F.cross_entropy(output.squeeze().transpose(0, 1), labels.squeeze(), size_average=False)\n        # Weighted loss. Typical utterance has 72 phonemes\n        # L2 REGULARIZATION\n        if args.cuda:\n             l2_reg = Variable(torch.cuda.FloatTensor(1), requires_grad=True)\n        else:\n             l2_reg = Variable(torch.FloatTensor(1), requires_grad=True)\n        for W in model.parameters():\n             l2_reg = l2_reg + W.norm(2)\n        l2_reg=l2_reg.squeeze()\n        loss += args.l2_reg * l2_reg\n        weighted_loss = loss * n_phones / 72.0\n        weighted_loss.backward()\n        optimizer.step()\n        # average loss per phoneme\n        avg_loss = loss / n_phones\n\n\n        if batch_idx % args.log_interval == 0:\n\n            print('Train Epoch: {} Batch: {} [{}/{} ({:.2f}%, time:{:.2f}s)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), time.time() - t0,\n                avg_loss.data))\n            t0 = time.time()\n\n\ndef test(model, test_loader, args):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    for frames, bounds, labels in test_loader:\n        if args.cuda:\n            frames, bounds, labels = map(lambda x: x.cuda(), [frames, bounds, labels])\n        frames, bounds, labels = Variable(frames, volatile=True), Variable(bounds), Variable(labels)\n\n        data = frames\n\n        output = model(data, bounds=bounds)\n        output = output.squeeze().transpose(0, 1)\n        labels = labels.squeeze()\n        test_loss += F.cross_entropy(output, labels, size_average=False).data  # sum up batch loss\n        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n        correct += pred.eq(labels.data.view_as(pred)).cpu().sum()\n\n    test_loss /= test_loader.dataset.total_phonemes\n    accuracy = correct / test_loader.dataset.total_phonemes\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.4f}%)\\n'.format(\n        test_loss, correct, test_loader.dataset.total_phonemes,\n        100 * accuracy))\n    return \"{:.4f}%\".format(100. * correct / test_loader.dataset.total_phonemes), accuracy\n\n\ndef main(args):\n    print(args)\n\n    torch.manual_seed(args.seed)\n    if args.cuda:\n        torch.cuda.manual_seed(args.seed)\n\n    train_loader, test_loader = get_data_loaders(args, for_conv2d=False)\n\n    model = MyModelResNet1D()\n\n    if args.cuda:\n        model.cuda()\n\n\n    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=5e-5) #1e-4\n    scheduler = ReduceLROnPlateau(optimizer, 'max', factor=0.5, patience=2, verbose=True,\n                                  threshold_mode='abs', threshold=0.01, min_lr=1e-6)\n    for epoch in range(1, args.epochs + 1):\n        print(datetime.now())\n        train(epoch, model, optimizer, train_loader, args)\n        acc_str, acc = test(model, test_loader, args)\n        scheduler.step(acc)\n        if not os.path.exists(args.weights_dir):\n            os.makedirs(args.weights_dir)\n        torch.save(model.state_dict(), \"{}/{:03d}_{}.w\".format(args.weights_dir, epoch, acc_str))\n\n\ndef predict_batch(model, x, bounds, args):\n    if args.cuda:\n        model.cuda()\n        x = x.cuda()\n        bounds = bounds.cuda()\n    model.eval()\n    output = model(Variable(x, volatile=True), bounds=Variable(bounds))\n    output = output.squeeze().transpose(0, 1)\n    return output.data.max(1, keepdim=True)[1]\n\n\ndef get_test_data_loaders(args):\n    print(\"loading data\")\n    xtest = np.load(args.data_dir + '/test_data.npy',allow_pickle=True)\n\n    print(\"load complete\")\n    kwargs = {'pin_memory': True} if args.cuda else {}\n    test_loader = torch.utils.data.DataLoader(\n        MyDataset(xtest, None),\n        batch_size=args.batch_size, shuffle=False, **kwargs)\n    return test_loader\n\n\ndef predict(args, csv_fpath, weights_fpath):\n    model = MyModelResNet1D()\n    model.load_state_dict(torch.load(weights_fpath))\n    test_loader = get_test_data_loaders(args)\n    with open(csv_fpath, 'w') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=['Id', 'Label'])\n        writer.writeheader()\n        cnt = 0\n        for batch, (frames, bounds, _) in enumerate(test_loader):\n            if batch % args.log_interval == 0:\n                print(\"batch\", batch)\n            yhat = predict_batch(model, frames, bounds, args)\n            for i, y in enumerate(yhat[:]):\n                writer.writerow({\"Id\": cnt + i, \"Label\": y.cpu()[0]})\n            cnt += len(yhat)\n    print(\"done\")","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:24:20.252525Z","iopub.execute_input":"2023-04-04T16:24:20.253156Z","iopub.status.idle":"2023-04-04T16:24:20.298472Z","shell.execute_reply.started":"2023-04-04T16:24:20.253118Z","shell.execute_reply":"2023-04-04T16:24:20.297378Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\nparser.add_argument('--batch-size', type=int, default=32, metavar='N',\n                    help='input batch size for training (default: 64)')\nparser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                    help='input batch size for testing (default: 1000)')\nparser.add_argument('--epochs', type=int, default=10, metavar='N',\n                    help='number of epochs to train (default: 10)')\nparser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n                    help='learning rate (default: 0.001)')\nparser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n                    help='SGD momentum (default: 0.9)')\nparser.add_argument('--l2-reg', type=float, default=0.001,\n                    help='l2 regularization')\nparser.add_argument('--no-cuda', action='store_true', default=False,\n                    help='disables CUDA training')\nparser.add_argument('--seed', type=int, default=1, metavar='S',\n                    help='random seed (default: 1)')\nparser.add_argument('--log-interval', type=int, default=500, metavar='N',\n                    help='how many batches to wait before logging training status')\nparser.add_argument('--data-dir', type=str, default='/kaggle/input/idc-410-spring-2023',\n                    help='data directory')\nparser.add_argument('--weights-dir', type=str, default='./weights/',\n                    help='data directory')","metadata":{"execution":{"iopub.status.busy":"2023-04-01T21:47:06.740896Z","iopub.execute_input":"2023-04-01T21:47:06.741896Z","iopub.status.idle":"2023-04-01T21:47:06.760233Z","shell.execute_reply.started":"2023-04-01T21:47:06.741822Z","shell.execute_reply":"2023-04-01T21:47:06.758938Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"_StoreAction(option_strings=['--weights-dir'], dest='weights_dir', nargs=None, const=None, default='./weights/', type=<class 'str'>, choices=None, help='data directory', metavar=None)"},"metadata":{}}]},{"cell_type":"code","source":"import argparse\nif __name__ == \"__main__\":\n    print(torch.__version__)\n    args, unknown = parser.parse_known_args()\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n    args.batch_size = 1\n    main(args)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T21:47:07.363328Z","iopub.execute_input":"2023-04-01T21:47:07.364226Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"1.13.0\nNamespace(batch_size=1, cuda=True, data_dir='/kaggle/input/idc-410-spring-2023', epochs=10, l2_reg=0.001, log_interval=500, lr=0.001, momentum=0.9, no_cuda=False, seed=1, test_batch_size=1000, weights_dir='./weights/')\nloading data\nload complete\nn_utters 15491, total_phonemes 1204725\nn_utters 1722, total_phonemes 133209\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n","output_type":"stream"},{"name":"stdout","text":"2023-04-01 21:47:26.746777\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n  warnings.warn(warning.format(ret))\n","output_type":"stream"},{"name":"stdout","text":"Train Epoch: 1 Batch: 0 [0/15491 (0.00%, time:5.24s)]\tLoss: 4.047507\nTrain Epoch: 1 Batch: 500 [500/15491 (3.23%, time:22.53s)]\tLoss: 1.101856\nTrain Epoch: 1 Batch: 1000 [1000/15491 (6.46%, time:21.51s)]\tLoss: 1.556414\nTrain Epoch: 1 Batch: 1500 [1500/15491 (9.68%, time:21.78s)]\tLoss: 0.883833\nTrain Epoch: 1 Batch: 2000 [2000/15491 (12.91%, time:21.43s)]\tLoss: 0.844787\nTrain Epoch: 1 Batch: 2500 [2500/15491 (16.14%, time:21.22s)]\tLoss: 1.005906\nTrain Epoch: 1 Batch: 3000 [3000/15491 (19.37%, time:21.82s)]\tLoss: 5.629163\nTrain Epoch: 1 Batch: 3500 [3500/15491 (22.59%, time:21.57s)]\tLoss: 1.162315\nTrain Epoch: 1 Batch: 4000 [4000/15491 (25.82%, time:20.90s)]\tLoss: 0.899623\nTrain Epoch: 1 Batch: 4500 [4500/15491 (29.05%, time:21.12s)]\tLoss: 1.084837\nTrain Epoch: 1 Batch: 5000 [5000/15491 (32.28%, time:21.25s)]\tLoss: 0.720578\nTrain Epoch: 1 Batch: 5500 [5500/15491 (35.50%, time:20.98s)]\tLoss: 0.378615\nTrain Epoch: 1 Batch: 6000 [6000/15491 (38.73%, time:22.06s)]\tLoss: 1.189397\nTrain Epoch: 1 Batch: 6500 [6500/15491 (41.96%, time:21.88s)]\tLoss: 0.602917\nTrain Epoch: 1 Batch: 7000 [7000/15491 (45.19%, time:21.68s)]\tLoss: 0.583383\nTrain Epoch: 1 Batch: 7500 [7500/15491 (48.42%, time:22.67s)]\tLoss: 0.941559\nTrain Epoch: 1 Batch: 8000 [8000/15491 (51.64%, time:21.89s)]\tLoss: 0.879362\nTrain Epoch: 1 Batch: 8500 [8500/15491 (54.87%, time:22.51s)]\tLoss: 2.000809\nTrain Epoch: 1 Batch: 9000 [9000/15491 (58.10%, time:21.82s)]\tLoss: 0.821536\nTrain Epoch: 1 Batch: 9500 [9500/15491 (61.33%, time:22.16s)]\tLoss: 0.538223\nTrain Epoch: 1 Batch: 10000 [10000/15491 (64.55%, time:22.05s)]\tLoss: 0.933538\nTrain Epoch: 1 Batch: 10500 [10500/15491 (67.78%, time:21.96s)]\tLoss: 0.960032\nTrain Epoch: 1 Batch: 11000 [11000/15491 (71.01%, time:22.40s)]\tLoss: 0.537447\nTrain Epoch: 1 Batch: 11500 [11500/15491 (74.24%, time:22.30s)]\tLoss: 0.855656\nTrain Epoch: 1 Batch: 12000 [12000/15491 (77.46%, time:21.51s)]\tLoss: 0.457760\nTrain Epoch: 1 Batch: 12500 [12500/15491 (80.69%, time:22.05s)]\tLoss: 0.732026\nTrain Epoch: 1 Batch: 13000 [13000/15491 (83.92%, time:22.42s)]\tLoss: 0.974599\nTrain Epoch: 1 Batch: 13500 [13500/15491 (87.15%, time:21.75s)]\tLoss: 0.868821\nTrain Epoch: 1 Batch: 14000 [14000/15491 (90.38%, time:21.33s)]\tLoss: 0.558073\nTrain Epoch: 1 Batch: 14500 [14500/15491 (93.60%, time:21.84s)]\tLoss: 1.225353\nTrain Epoch: 1 Batch: 15000 [15000/15491 (96.83%, time:21.92s)]\tLoss: 0.786858\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:177: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.9629, Accuracy: 100406/133209 (75.3748%)\n\n2023-04-01 21:59:06.729368\nTrain Epoch: 2 Batch: 0 [0/15491 (0.00%, time:0.24s)]\tLoss: 1.346804\nTrain Epoch: 2 Batch: 500 [500/15491 (3.23%, time:22.36s)]\tLoss: 0.880852\nTrain Epoch: 2 Batch: 1000 [1000/15491 (6.46%, time:21.41s)]\tLoss: 0.374020\nTrain Epoch: 2 Batch: 1500 [1500/15491 (9.68%, time:22.36s)]\tLoss: 0.600757\nTrain Epoch: 2 Batch: 2000 [2000/15491 (12.91%, time:21.96s)]\tLoss: 0.501119\nTrain Epoch: 2 Batch: 2500 [2500/15491 (16.14%, time:21.50s)]\tLoss: 0.660453\nTrain Epoch: 2 Batch: 3000 [3000/15491 (19.37%, time:22.11s)]\tLoss: 0.749095\nTrain Epoch: 2 Batch: 3500 [3500/15491 (22.59%, time:21.88s)]\tLoss: 0.844517\nTrain Epoch: 2 Batch: 4000 [4000/15491 (25.82%, time:21.69s)]\tLoss: 0.723685\nTrain Epoch: 2 Batch: 4500 [4500/15491 (29.05%, time:22.21s)]\tLoss: 0.796879\nTrain Epoch: 2 Batch: 5000 [5000/15491 (32.28%, time:22.22s)]\tLoss: 0.943791\nTrain Epoch: 2 Batch: 5500 [5500/15491 (35.50%, time:22.11s)]\tLoss: 0.901391\nTrain Epoch: 2 Batch: 6000 [6000/15491 (38.73%, time:21.88s)]\tLoss: 1.493755\nTrain Epoch: 2 Batch: 6500 [6500/15491 (41.96%, time:22.05s)]\tLoss: 0.437856\nTrain Epoch: 2 Batch: 7000 [7000/15491 (45.19%, time:22.33s)]\tLoss: 0.394444\nTrain Epoch: 2 Batch: 7500 [7500/15491 (48.42%, time:21.33s)]\tLoss: 3.975513\nTrain Epoch: 2 Batch: 8000 [8000/15491 (51.64%, time:21.84s)]\tLoss: 0.249096\nTrain Epoch: 2 Batch: 8500 [8500/15491 (54.87%, time:22.17s)]\tLoss: 0.591989\nTrain Epoch: 2 Batch: 9000 [9000/15491 (58.10%, time:22.05s)]\tLoss: 0.337927\nTrain Epoch: 2 Batch: 9500 [9500/15491 (61.33%, time:21.94s)]\tLoss: 0.817024\nTrain Epoch: 2 Batch: 10000 [10000/15491 (64.55%, time:22.35s)]\tLoss: 1.314587\nTrain Epoch: 2 Batch: 10500 [10500/15491 (67.78%, time:21.64s)]\tLoss: 0.784783\nTrain Epoch: 2 Batch: 11000 [11000/15491 (71.01%, time:22.02s)]\tLoss: 0.894408\nTrain Epoch: 2 Batch: 11500 [11500/15491 (74.24%, time:21.98s)]\tLoss: 0.328703\nTrain Epoch: 2 Batch: 12000 [12000/15491 (77.46%, time:21.78s)]\tLoss: 1.259643\nTrain Epoch: 2 Batch: 12500 [12500/15491 (80.69%, time:22.16s)]\tLoss: 0.475084\nTrain Epoch: 2 Batch: 13000 [13000/15491 (83.92%, time:21.85s)]\tLoss: 0.813535\nTrain Epoch: 2 Batch: 13500 [13500/15491 (87.15%, time:22.00s)]\tLoss: 0.982439\nTrain Epoch: 2 Batch: 14000 [14000/15491 (90.38%, time:21.59s)]\tLoss: 0.802844\nTrain Epoch: 2 Batch: 14500 [14500/15491 (93.60%, time:21.90s)]\tLoss: 1.132415\nTrain Epoch: 2 Batch: 15000 [15000/15491 (96.83%, time:21.94s)]\tLoss: 0.720814\n\nTest set: Average loss: 0.8890, Accuracy: 103821/133209 (77.9384%)\n\n2023-04-01 22:10:46.401103\nTrain Epoch: 3 Batch: 0 [0/15491 (0.00%, time:0.23s)]\tLoss: 0.643558\nTrain Epoch: 3 Batch: 500 [500/15491 (3.23%, time:22.23s)]\tLoss: 0.337084\nTrain Epoch: 3 Batch: 1000 [1000/15491 (6.46%, time:21.22s)]\tLoss: 0.661056\nTrain Epoch: 3 Batch: 1500 [1500/15491 (9.68%, time:21.71s)]\tLoss: 0.249838\nTrain Epoch: 3 Batch: 2000 [2000/15491 (12.91%, time:21.85s)]\tLoss: 0.552832\nTrain Epoch: 3 Batch: 2500 [2500/15491 (16.14%, time:22.25s)]\tLoss: 0.677667\nTrain Epoch: 3 Batch: 3000 [3000/15491 (19.37%, time:22.63s)]\tLoss: 0.427420\nTrain Epoch: 3 Batch: 3500 [3500/15491 (22.59%, time:22.81s)]\tLoss: 0.499943\nTrain Epoch: 3 Batch: 4000 [4000/15491 (25.82%, time:22.58s)]\tLoss: 0.615966\nTrain Epoch: 3 Batch: 4500 [4500/15491 (29.05%, time:22.52s)]\tLoss: 0.650223\nTrain Epoch: 3 Batch: 5000 [5000/15491 (32.28%, time:23.18s)]\tLoss: 0.328054\nTrain Epoch: 3 Batch: 5500 [5500/15491 (35.50%, time:22.87s)]\tLoss: 0.378565\nTrain Epoch: 3 Batch: 6000 [6000/15491 (38.73%, time:21.98s)]\tLoss: 0.505852\nTrain Epoch: 3 Batch: 6500 [6500/15491 (41.96%, time:22.55s)]\tLoss: 0.435186\nTrain Epoch: 3 Batch: 7000 [7000/15491 (45.19%, time:22.14s)]\tLoss: 0.741067\nTrain Epoch: 3 Batch: 7500 [7500/15491 (48.42%, time:22.67s)]\tLoss: 0.854590\nTrain Epoch: 3 Batch: 8000 [8000/15491 (51.64%, time:21.80s)]\tLoss: 0.242281\nTrain Epoch: 3 Batch: 8500 [8500/15491 (54.87%, time:22.42s)]\tLoss: 0.364317\nTrain Epoch: 3 Batch: 9000 [9000/15491 (58.10%, time:22.10s)]\tLoss: 0.521996\nTrain Epoch: 3 Batch: 9500 [9500/15491 (61.33%, time:22.24s)]\tLoss: 0.462707\nTrain Epoch: 3 Batch: 10000 [10000/15491 (64.55%, time:22.37s)]\tLoss: 0.610001\nTrain Epoch: 3 Batch: 10500 [10500/15491 (67.78%, time:22.89s)]\tLoss: 0.407040\nTrain Epoch: 3 Batch: 11000 [11000/15491 (71.01%, time:21.64s)]\tLoss: 0.441780\nTrain Epoch: 3 Batch: 11500 [11500/15491 (74.24%, time:22.63s)]\tLoss: 0.933775\nTrain Epoch: 3 Batch: 12000 [12000/15491 (77.46%, time:22.87s)]\tLoss: 0.862676\nTrain Epoch: 3 Batch: 12500 [12500/15491 (80.69%, time:22.71s)]\tLoss: 0.462736\nTrain Epoch: 3 Batch: 13000 [13000/15491 (83.92%, time:22.02s)]\tLoss: 0.522585\nTrain Epoch: 3 Batch: 13500 [13500/15491 (87.15%, time:23.03s)]\tLoss: 0.250437\nTrain Epoch: 3 Batch: 14000 [14000/15491 (90.38%, time:22.94s)]\tLoss: 0.430123\nTrain Epoch: 3 Batch: 14500 [14500/15491 (93.60%, time:21.83s)]\tLoss: 0.761502\nTrain Epoch: 3 Batch: 15000 [15000/15491 (96.83%, time:22.67s)]\tLoss: 1.096131\n\nTest set: Average loss: 0.8461, Accuracy: 105640/133209 (79.3039%)\n\n2023-04-01 22:22:40.021422\nTrain Epoch: 4 Batch: 0 [0/15491 (0.00%, time:0.23s)]\tLoss: 0.337617\nTrain Epoch: 4 Batch: 500 [500/15491 (3.23%, time:22.43s)]\tLoss: 0.228892\nTrain Epoch: 4 Batch: 1000 [1000/15491 (6.46%, time:22.11s)]\tLoss: 0.492338\nTrain Epoch: 4 Batch: 1500 [1500/15491 (9.68%, time:22.99s)]\tLoss: 0.361982\nTrain Epoch: 4 Batch: 2000 [2000/15491 (12.91%, time:22.07s)]\tLoss: 0.477260\nTrain Epoch: 4 Batch: 2500 [2500/15491 (16.14%, time:22.79s)]\tLoss: 0.414729\nTrain Epoch: 4 Batch: 3000 [3000/15491 (19.37%, time:22.29s)]\tLoss: 0.744890\nTrain Epoch: 4 Batch: 3500 [3500/15491 (22.59%, time:22.14s)]\tLoss: 0.320910\nTrain Epoch: 4 Batch: 4000 [4000/15491 (25.82%, time:22.98s)]\tLoss: 0.443950\nTrain Epoch: 4 Batch: 4500 [4500/15491 (29.05%, time:22.63s)]\tLoss: 0.376074\nTrain Epoch: 4 Batch: 5000 [5000/15491 (32.28%, time:22.28s)]\tLoss: 0.307929\nTrain Epoch: 4 Batch: 5500 [5500/15491 (35.50%, time:21.79s)]\tLoss: 0.632348\nTrain Epoch: 4 Batch: 6000 [6000/15491 (38.73%, time:22.58s)]\tLoss: 0.569188\nTrain Epoch: 4 Batch: 6500 [6500/15491 (41.96%, time:22.35s)]\tLoss: 0.216541\nTrain Epoch: 4 Batch: 7000 [7000/15491 (45.19%, time:21.98s)]\tLoss: 3.071228\nTrain Epoch: 4 Batch: 7500 [7500/15491 (48.42%, time:22.54s)]\tLoss: 0.323666\nTrain Epoch: 4 Batch: 8000 [8000/15491 (51.64%, time:22.83s)]\tLoss: 0.998698\nTrain Epoch: 4 Batch: 8500 [8500/15491 (54.87%, time:22.04s)]\tLoss: 0.685421\nTrain Epoch: 4 Batch: 9000 [9000/15491 (58.10%, time:22.24s)]\tLoss: 0.604996\nTrain Epoch: 4 Batch: 9500 [9500/15491 (61.33%, time:22.39s)]\tLoss: 0.340000\nTrain Epoch: 4 Batch: 10000 [10000/15491 (64.55%, time:22.56s)]\tLoss: 0.823372\nTrain Epoch: 4 Batch: 10500 [10500/15491 (67.78%, time:21.83s)]\tLoss: 0.607199\nTrain Epoch: 4 Batch: 11000 [11000/15491 (71.01%, time:22.62s)]\tLoss: 0.271327\nTrain Epoch: 4 Batch: 11500 [11500/15491 (74.24%, time:22.09s)]\tLoss: 0.274437\nTrain Epoch: 4 Batch: 12000 [12000/15491 (77.46%, time:21.78s)]\tLoss: 0.166240\nTrain Epoch: 4 Batch: 12500 [12500/15491 (80.69%, time:21.53s)]\tLoss: 0.291637\nTrain Epoch: 4 Batch: 13000 [13000/15491 (83.92%, time:22.32s)]\tLoss: 0.503156\nTrain Epoch: 4 Batch: 13500 [13500/15491 (87.15%, time:21.31s)]\tLoss: 0.997299\nTrain Epoch: 4 Batch: 14000 [14000/15491 (90.38%, time:21.72s)]\tLoss: 0.554680\nTrain Epoch: 4 Batch: 14500 [14500/15491 (93.60%, time:21.29s)]\tLoss: 0.286486\nTrain Epoch: 4 Batch: 15000 [15000/15491 (96.83%, time:22.23s)]\tLoss: 0.513216\n\nTest set: Average loss: 0.8302, Accuracy: 106944/133209 (80.2829%)\n\n2023-04-01 22:34:28.274992\nTrain Epoch: 5 Batch: 0 [0/15491 (0.00%, time:0.23s)]\tLoss: 4.346270\nTrain Epoch: 5 Batch: 500 [500/15491 (3.23%, time:22.41s)]\tLoss: 0.331539\nTrain Epoch: 5 Batch: 1000 [1000/15491 (6.46%, time:21.48s)]\tLoss: 0.862113\nTrain Epoch: 5 Batch: 1500 [1500/15491 (9.68%, time:21.94s)]\tLoss: 1.134120\nTrain Epoch: 5 Batch: 2000 [2000/15491 (12.91%, time:22.12s)]\tLoss: 0.235339\nTrain Epoch: 5 Batch: 2500 [2500/15491 (16.14%, time:21.41s)]\tLoss: 0.308011\nTrain Epoch: 5 Batch: 3000 [3000/15491 (19.37%, time:21.69s)]\tLoss: 0.311763\nTrain Epoch: 5 Batch: 3500 [3500/15491 (22.59%, time:21.45s)]\tLoss: 0.442490\nTrain Epoch: 5 Batch: 4000 [4000/15491 (25.82%, time:21.05s)]\tLoss: 0.535661\nTrain Epoch: 5 Batch: 4500 [4500/15491 (29.05%, time:21.67s)]\tLoss: 0.517868\nTrain Epoch: 5 Batch: 5000 [5000/15491 (32.28%, time:21.70s)]\tLoss: 0.259763\nTrain Epoch: 5 Batch: 5500 [5500/15491 (35.50%, time:21.66s)]\tLoss: 0.602305\nTrain Epoch: 5 Batch: 6000 [6000/15491 (38.73%, time:22.23s)]\tLoss: 0.896023\nTrain Epoch: 5 Batch: 6500 [6500/15491 (41.96%, time:22.40s)]\tLoss: 0.246321\nTrain Epoch: 5 Batch: 7000 [7000/15491 (45.19%, time:21.36s)]\tLoss: 0.221909\nTrain Epoch: 5 Batch: 7500 [7500/15491 (48.42%, time:22.05s)]\tLoss: 0.225692\nTrain Epoch: 5 Batch: 8000 [8000/15491 (51.64%, time:22.47s)]\tLoss: 0.476852\nTrain Epoch: 5 Batch: 8500 [8500/15491 (54.87%, time:23.19s)]\tLoss: 0.546571\nTrain Epoch: 5 Batch: 9000 [9000/15491 (58.10%, time:22.33s)]\tLoss: 0.289735\nTrain Epoch: 5 Batch: 9500 [9500/15491 (61.33%, time:23.20s)]\tLoss: 0.426828\nTrain Epoch: 5 Batch: 10000 [10000/15491 (64.55%, time:22.43s)]\tLoss: 0.569220\nTrain Epoch: 5 Batch: 10500 [10500/15491 (67.78%, time:22.06s)]\tLoss: 0.248532\nTrain Epoch: 5 Batch: 11000 [11000/15491 (71.01%, time:22.70s)]\tLoss: 0.407038\nTrain Epoch: 5 Batch: 11500 [11500/15491 (74.24%, time:22.71s)]\tLoss: 0.378634\nTrain Epoch: 5 Batch: 12000 [12000/15491 (77.46%, time:22.50s)]\tLoss: 0.502586\nTrain Epoch: 5 Batch: 12500 [12500/15491 (80.69%, time:22.42s)]\tLoss: 0.824425\nTrain Epoch: 5 Batch: 13000 [13000/15491 (83.92%, time:22.57s)]\tLoss: 0.878599\nTrain Epoch: 5 Batch: 13500 [13500/15491 (87.15%, time:23.07s)]\tLoss: 0.264183\nTrain Epoch: 5 Batch: 14000 [14000/15491 (90.38%, time:22.70s)]\tLoss: 0.211571\nTrain Epoch: 5 Batch: 14500 [14500/15491 (93.60%, time:23.15s)]\tLoss: 0.578107\nTrain Epoch: 5 Batch: 15000 [15000/15491 (96.83%, time:22.83s)]\tLoss: 0.988030\n\nTest set: Average loss: 0.8384, Accuracy: 105953/133209 (79.5389%)\n\n2023-04-01 22:46:17.391694\nTrain Epoch: 6 Batch: 0 [0/15491 (0.00%, time:0.27s)]\tLoss: 0.665722\nTrain Epoch: 6 Batch: 500 [500/15491 (3.23%, time:23.19s)]\tLoss: 1.740481\nTrain Epoch: 6 Batch: 1000 [1000/15491 (6.46%, time:22.74s)]\tLoss: 0.496274\nTrain Epoch: 6 Batch: 1500 [1500/15491 (9.68%, time:22.25s)]\tLoss: 0.218088\nTrain Epoch: 6 Batch: 2000 [2000/15491 (12.91%, time:22.81s)]\tLoss: 0.252065\nTrain Epoch: 6 Batch: 2500 [2500/15491 (16.14%, time:22.65s)]\tLoss: 0.449291\nTrain Epoch: 6 Batch: 3000 [3000/15491 (19.37%, time:22.59s)]\tLoss: 0.591768\nTrain Epoch: 6 Batch: 3500 [3500/15491 (22.59%, time:22.22s)]\tLoss: 0.588369\nTrain Epoch: 6 Batch: 4000 [4000/15491 (25.82%, time:22.54s)]\tLoss: 0.654775\nTrain Epoch: 6 Batch: 4500 [4500/15491 (29.05%, time:22.84s)]\tLoss: 0.420299\nTrain Epoch: 6 Batch: 5000 [5000/15491 (32.28%, time:22.04s)]\tLoss: 0.453424\nTrain Epoch: 6 Batch: 5500 [5500/15491 (35.50%, time:22.86s)]\tLoss: 0.893944\nTrain Epoch: 6 Batch: 6000 [6000/15491 (38.73%, time:22.46s)]\tLoss: 0.330994\nTrain Epoch: 6 Batch: 6500 [6500/15491 (41.96%, time:22.04s)]\tLoss: 0.338702\nTrain Epoch: 6 Batch: 7000 [7000/15491 (45.19%, time:22.67s)]\tLoss: 0.475488\nTrain Epoch: 6 Batch: 7500 [7500/15491 (48.42%, time:22.50s)]\tLoss: 0.197612\nTrain Epoch: 6 Batch: 8000 [8000/15491 (51.64%, time:22.36s)]\tLoss: 0.392035\nTrain Epoch: 6 Batch: 8500 [8500/15491 (54.87%, time:22.04s)]\tLoss: 0.450330\nTrain Epoch: 6 Batch: 9000 [9000/15491 (58.10%, time:22.26s)]\tLoss: 0.397706\nTrain Epoch: 6 Batch: 9500 [9500/15491 (61.33%, time:22.64s)]\tLoss: 0.617517\nTrain Epoch: 6 Batch: 10000 [10000/15491 (64.55%, time:22.00s)]\tLoss: 0.329726\nTrain Epoch: 6 Batch: 10500 [10500/15491 (67.78%, time:22.46s)]\tLoss: 0.458329\nTrain Epoch: 6 Batch: 11000 [11000/15491 (71.01%, time:22.90s)]\tLoss: 1.001523\nTrain Epoch: 6 Batch: 11500 [11500/15491 (74.24%, time:23.01s)]\tLoss: 0.323659\nTrain Epoch: 6 Batch: 12000 [12000/15491 (77.46%, time:22.25s)]\tLoss: 0.451462\nTrain Epoch: 6 Batch: 12500 [12500/15491 (80.69%, time:22.72s)]\tLoss: 1.899769\nTrain Epoch: 6 Batch: 13000 [13000/15491 (83.92%, time:22.66s)]\tLoss: 0.246293\nTrain Epoch: 6 Batch: 13500 [13500/15491 (87.15%, time:22.23s)]\tLoss: 0.182008\nTrain Epoch: 6 Batch: 14000 [14000/15491 (90.38%, time:22.57s)]\tLoss: 0.367905\nTrain Epoch: 6 Batch: 14500 [14500/15491 (93.60%, time:22.63s)]\tLoss: 0.495843\nTrain Epoch: 6 Batch: 15000 [15000/15491 (96.83%, time:22.72s)]\tLoss: 0.609897\n\nTest set: Average loss: 0.8593, Accuracy: 107824/133209 (80.9435%)\n\n2023-04-01 22:58:15.503485\nTrain Epoch: 7 Batch: 0 [0/15491 (0.00%, time:0.24s)]\tLoss: 0.729513\nTrain Epoch: 7 Batch: 500 [500/15491 (3.23%, time:22.88s)]\tLoss: 0.215353\nTrain Epoch: 7 Batch: 1000 [1000/15491 (6.46%, time:21.99s)]\tLoss: 0.443077\nTrain Epoch: 7 Batch: 1500 [1500/15491 (9.68%, time:22.80s)]\tLoss: 0.769114\nTrain Epoch: 7 Batch: 2000 [2000/15491 (12.91%, time:22.91s)]\tLoss: 0.434289\nTrain Epoch: 7 Batch: 2500 [2500/15491 (16.14%, time:22.32s)]\tLoss: 0.886651\nTrain Epoch: 7 Batch: 3000 [3000/15491 (19.37%, time:22.42s)]\tLoss: 0.426703\nTrain Epoch: 7 Batch: 3500 [3500/15491 (22.59%, time:22.79s)]\tLoss: 0.316563\nTrain Epoch: 7 Batch: 4000 [4000/15491 (25.82%, time:22.55s)]\tLoss: 0.657169\nTrain Epoch: 7 Batch: 4500 [4500/15491 (29.05%, time:21.92s)]\tLoss: 0.189855\nTrain Epoch: 7 Batch: 5000 [5000/15491 (32.28%, time:22.26s)]\tLoss: 0.422563\nTrain Epoch: 7 Batch: 5500 [5500/15491 (35.50%, time:22.51s)]\tLoss: 0.313579\nTrain Epoch: 7 Batch: 6000 [6000/15491 (38.73%, time:21.94s)]\tLoss: 0.436858\nTrain Epoch: 7 Batch: 6500 [6500/15491 (41.96%, time:22.55s)]\tLoss: 0.210961\nTrain Epoch: 7 Batch: 7000 [7000/15491 (45.19%, time:22.54s)]\tLoss: 0.287547\nTrain Epoch: 7 Batch: 7500 [7500/15491 (48.42%, time:21.75s)]\tLoss: 0.684862\nTrain Epoch: 7 Batch: 8000 [8000/15491 (51.64%, time:22.37s)]\tLoss: 0.454192\nTrain Epoch: 7 Batch: 8500 [8500/15491 (54.87%, time:22.36s)]\tLoss: 0.359094\nTrain Epoch: 7 Batch: 9000 [9000/15491 (58.10%, time:22.27s)]\tLoss: 3.412357\nTrain Epoch: 7 Batch: 9500 [9500/15491 (61.33%, time:21.89s)]\tLoss: 3.973621\nTrain Epoch: 7 Batch: 10000 [10000/15491 (64.55%, time:22.69s)]\tLoss: 0.343071\nTrain Epoch: 7 Batch: 10500 [10500/15491 (67.78%, time:22.36s)]\tLoss: 0.363760\nTrain Epoch: 7 Batch: 11000 [11000/15491 (71.01%, time:22.47s)]\tLoss: 0.422114\nTrain Epoch: 7 Batch: 11500 [11500/15491 (74.24%, time:22.37s)]\tLoss: 0.556530\nTrain Epoch: 7 Batch: 12000 [12000/15491 (77.46%, time:22.31s)]\tLoss: 0.554256\nTrain Epoch: 7 Batch: 12500 [12500/15491 (80.69%, time:22.29s)]\tLoss: 1.648664\nTrain Epoch: 7 Batch: 13000 [13000/15491 (83.92%, time:21.75s)]\tLoss: 0.182235\nTrain Epoch: 7 Batch: 13500 [13500/15491 (87.15%, time:22.45s)]\tLoss: 0.237135\nTrain Epoch: 7 Batch: 14000 [14000/15491 (90.38%, time:22.16s)]\tLoss: 0.215222\nTrain Epoch: 7 Batch: 14500 [14500/15491 (93.60%, time:21.77s)]\tLoss: 0.551095\nTrain Epoch: 7 Batch: 15000 [15000/15491 (96.83%, time:21.95s)]\tLoss: 0.290530\n\nTest set: Average loss: 0.8108, Accuracy: 107749/133209 (80.8872%)\n\n2023-04-01 23:10:06.018891\nTrain Epoch: 8 Batch: 0 [0/15491 (0.00%, time:0.24s)]\tLoss: 0.468826\nTrain Epoch: 8 Batch: 500 [500/15491 (3.23%, time:22.53s)]\tLoss: 0.280317\nTrain Epoch: 8 Batch: 1000 [1000/15491 (6.46%, time:22.41s)]\tLoss: 0.248853\nTrain Epoch: 8 Batch: 1500 [1500/15491 (9.68%, time:21.91s)]\tLoss: 0.615683\nTrain Epoch: 8 Batch: 2000 [2000/15491 (12.91%, time:22.10s)]\tLoss: 0.322277\nTrain Epoch: 8 Batch: 2500 [2500/15491 (16.14%, time:22.19s)]\tLoss: 0.287834\nTrain Epoch: 8 Batch: 3000 [3000/15491 (19.37%, time:21.84s)]\tLoss: 0.239851\nTrain Epoch: 8 Batch: 3500 [3500/15491 (22.59%, time:21.83s)]\tLoss: 0.504886\nTrain Epoch: 8 Batch: 4000 [4000/15491 (25.82%, time:21.92s)]\tLoss: 0.390541\nTrain Epoch: 8 Batch: 4500 [4500/15491 (29.05%, time:21.74s)]\tLoss: 0.278202\nTrain Epoch: 8 Batch: 5000 [5000/15491 (32.28%, time:21.31s)]\tLoss: 0.427252\nTrain Epoch: 8 Batch: 5500 [5500/15491 (35.50%, time:22.36s)]\tLoss: 0.692863\nTrain Epoch: 8 Batch: 6000 [6000/15491 (38.73%, time:21.90s)]\tLoss: 0.273673\nTrain Epoch: 8 Batch: 6500 [6500/15491 (41.96%, time:21.25s)]\tLoss: 0.485313\nTrain Epoch: 8 Batch: 7000 [7000/15491 (45.19%, time:21.51s)]\tLoss: 0.300780\nTrain Epoch: 8 Batch: 7500 [7500/15491 (48.42%, time:21.74s)]\tLoss: 0.205470\nTrain Epoch: 8 Batch: 8000 [8000/15491 (51.64%, time:20.88s)]\tLoss: 1.002704\nTrain Epoch: 8 Batch: 8500 [8500/15491 (54.87%, time:21.50s)]\tLoss: 0.537960\nTrain Epoch: 8 Batch: 9000 [9000/15491 (58.10%, time:21.70s)]\tLoss: 0.362400\nTrain Epoch: 8 Batch: 9500 [9500/15491 (61.33%, time:21.05s)]\tLoss: 0.301221\nTrain Epoch: 8 Batch: 10000 [10000/15491 (64.55%, time:21.34s)]\tLoss: 0.123788\nTrain Epoch: 8 Batch: 10500 [10500/15491 (67.78%, time:21.38s)]\tLoss: 0.499295\nTrain Epoch: 8 Batch: 11000 [11000/15491 (71.01%, time:21.07s)]\tLoss: 0.362960\nTrain Epoch: 8 Batch: 11500 [11500/15491 (74.24%, time:21.08s)]\tLoss: 0.272249\nTrain Epoch: 8 Batch: 12000 [12000/15491 (77.46%, time:21.31s)]\tLoss: 0.311242\nTrain Epoch: 8 Batch: 12500 [12500/15491 (80.69%, time:20.76s)]\tLoss: 0.200022\nTrain Epoch: 8 Batch: 13000 [13000/15491 (83.92%, time:21.28s)]\tLoss: 2.433698\nTrain Epoch: 8 Batch: 13500 [13500/15491 (87.15%, time:21.21s)]\tLoss: 0.310253\nTrain Epoch: 8 Batch: 14000 [14000/15491 (90.38%, time:20.89s)]\tLoss: 0.827832\nTrain Epoch: 8 Batch: 14500 [14500/15491 (93.60%, time:21.23s)]\tLoss: 0.276467\nTrain Epoch: 8 Batch: 15000 [15000/15491 (96.83%, time:21.50s)]\tLoss: 0.188546\n\nTest set: Average loss: 0.8547, Accuracy: 108156/133209 (81.1927%)\n\n2023-04-01 23:21:32.697231\nTrain Epoch: 9 Batch: 0 [0/15491 (0.00%, time:0.23s)]\tLoss: 0.316224\nTrain Epoch: 9 Batch: 500 [500/15491 (3.23%, time:21.53s)]\tLoss: 0.342677\nTrain Epoch: 9 Batch: 1000 [1000/15491 (6.46%, time:20.92s)]\tLoss: 0.273037\nTrain Epoch: 9 Batch: 1500 [1500/15491 (9.68%, time:21.66s)]\tLoss: 0.607965\nTrain Epoch: 9 Batch: 2000 [2000/15491 (12.91%, time:21.74s)]\tLoss: 0.437430\nTrain Epoch: 9 Batch: 2500 [2500/15491 (16.14%, time:21.08s)]\tLoss: 0.208964\nTrain Epoch: 9 Batch: 3000 [3000/15491 (19.37%, time:21.59s)]\tLoss: 0.117638\nTrain Epoch: 9 Batch: 3500 [3500/15491 (22.59%, time:21.48s)]\tLoss: 0.608205\nTrain Epoch: 9 Batch: 4000 [4000/15491 (25.82%, time:21.04s)]\tLoss: 0.335910\nTrain Epoch: 9 Batch: 4500 [4500/15491 (29.05%, time:21.33s)]\tLoss: 0.292678\n","output_type":"stream"}]},{"cell_type":"code","source":"# It uses the weight file to calculate the labels and then save it as csv file\npredict(args, '/kaggle/working/submission.csv', '/kaggle/working/weights/001_75.4243%.w')","metadata":{"execution":{"iopub.status.busy":"2023-04-01T17:15:52.084142Z","iopub.execute_input":"2023-04-01T17:15:52.084532Z","iopub.status.idle":"2023-04-01T17:17:57.296436Z","shell.execute_reply.started":"2023-04-01T17:15:52.084497Z","shell.execute_reply":"2023-04-01T17:17:57.295262Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"loading data\nload complete\nn_utters 7377, total_phonemes 570973\nbatch 0\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:197: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","output_type":"stream"},{"name":"stdout","text":"batch 500\nbatch 1000\nbatch 1500\nbatch 2000\nbatch 2500\nbatch 3000\nbatch 3500\nbatch 4000\nbatch 4500\nbatch 5000\nbatch 5500\nbatch 6000\nbatch 6500\nbatch 7000\ndone\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv('/kaggle/working/submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-01T17:17:57.299724Z","iopub.execute_input":"2023-04-01T17:17:57.300581Z","iopub.status.idle":"2023-04-01T17:17:57.425914Z","shell.execute_reply.started":"2023-04-01T17:17:57.300534Z","shell.execute_reply":"2023-04-01T17:17:57.424843Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# this function convert the string values of labels to int.\ndef convert(df):\n    lst=[]\n    a=list(df[\"Label\"])\n    for i in range(len(a)):\n        if len(a[i])==10:\n            lst.append(int(a[i][7:9]))\n        else:\n            lst.append(int(a[i][7]))\n    return df[\"Id\"],lst","metadata":{"execution":{"iopub.status.busy":"2023-04-01T17:17:57.427783Z","iopub.execute_input":"2023-04-01T17:17:57.428273Z","iopub.status.idle":"2023-04-01T17:17:57.435265Z","shell.execute_reply.started":"2023-04-01T17:17:57.428234Z","shell.execute_reply":"2023-04-01T17:17:57.434044Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# here we are creating a new data frame and saving it as csv file\nid1,label=convert(df)\nlist_of_tuples = list(zip(id1, label))\ndf = pd.DataFrame(list_of_tuples,\n                  columns=['Id', 'Label'])\ndf.to_csv('output.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T17:17:57.438341Z","iopub.execute_input":"2023-04-01T17:17:57.439158Z","iopub.status.idle":"2023-04-01T17:17:58.125773Z","shell.execute_reply.started":"2023-04-01T17:17:57.439119Z","shell.execute_reply":"2023-04-01T17:17:58.124661Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-01T17:17:58.127244Z","iopub.execute_input":"2023-04-01T17:17:58.127700Z","iopub.status.idle":"2023-04-01T17:17:58.665383Z","shell.execute_reply.started":"2023-04-01T17:17:58.127653Z","shell.execute_reply":"2023-04-01T17:17:58.664304Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}